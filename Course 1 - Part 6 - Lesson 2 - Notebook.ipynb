{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%206%20-%20Lesson%202%20-%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rX8mhOLljYeM"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "BZSlp3DAjdYf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6gHiH-I7uFa"
   },
   "source": [
    "#Improving Computer Vision Accuracy using Convolutions\n",
    "\n",
    "In the previous lessons you saw how to do fashion recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with the impact of different sizes of hidden layer, number of training epochs etc on the final accuracy.\n",
    "\n",
    "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "height": 207
    },
    "colab_type": "code",
    "id": "xcsRtq9OLorS",
    "outputId": "027ddd16-b2d9-41a0-85aa-9da6275085e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4973 - accuracy: 0.8254\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.3773 - accuracy: 0.8655\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.3388 - accuracy: 0.8757\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.3148 - accuracy: 0.8850\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.3002 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.3563 - accuracy: 0.8746\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zldEXSsF8Noz"
   },
   "source": [
    "Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to details on Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, details. \n",
    "\n",
    "If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar.\n",
    "\n",
    "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\n",
    "\n",
    "This is perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on the highlighted features.\n",
    "\n",
    "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.\n",
    "\n",
    "Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "height": 605
    },
    "colab_type": "code",
    "id": "C0tFgT1MMKi6",
    "outputId": "b9c48f3c-639a-4c14-ebbe-657cacca81f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4455 - accuracy: 0.8375\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2993 - accuracy: 0.8901\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2527 - accuracy: 0.9057\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2189 - accuracy: 0.9176\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.1938 - accuracy: 0.9274\n",
      "10000/10000 [==============================] - 1s 61us/sample - loss: 0.2548 - accuracy: 0.9051\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss = model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRLfZ0jt-fQI"
   },
   "source": [
    "It's likely gone up to about 93% on the training data and 91% on the validation data. \n",
    "\n",
    "That's significant, and a step in the right direction!\n",
    "\n",
    "Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. \n",
    "\n",
    "(In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing *other* data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.)\n",
    "\n",
    "Then, look at the code again, and see, step by step how the Convolutions were built:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaLX5cgI_JDb"
   },
   "source": [
    "Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SS_W_INc_kJQ"
   },
   "source": [
    "Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\n",
    "\n",
    "1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
    "2. The size of the Convolution, in this case a 3x3 grid\n",
    "3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\n",
    "4. In the first layer, the shape of the input data.\n",
    "\n",
    "You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%.\n",
    "\n",
    "You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. \n",
    "\n",
    "\n",
    "```\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMorM6daADjA"
   },
   "source": [
    "Add another convolution\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1-x-kZF4_tC"
   },
   "source": [
    "Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
    "\n",
    "```\n",
    "  tf.keras.layers.Flatten(),\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPtqR23uASjX"
   },
   "source": [
    "The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C0GSsjUhAaSj"
   },
   "source": [
    "Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXx_LX3SAlFs"
   },
   "source": [
    "# Visualizing the Convolutions and Pooling\n",
    "\n",
    "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "f-6nX4QsOku6",
    "outputId": "6b85ed93-6868-4c2c-b066-0808d6536878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "9FGsHhv6JvDx",
    "outputId": "e144d639-cebc-4d0a-9c7a-8571f70d6159"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(3,4)\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=7\n",
    "THIRD_IMAGE=26\n",
    "CONVOLUTION_NUMBER = 1\n",
    "from tensorflow.keras import models\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "for x in range(0,4):\n",
    "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[0,x].grid(False)\n",
    "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[1,x].grid(False)\n",
    "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KVPZqgHo5Ux"
   },
   "source": [
    "EXERCISES\n",
    "\n",
    "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
    "\n",
    "    - Accuracy and training time both increase. Accuracy difference appears to be minimal in this example. Training time increased by ~10-20% from 16-64 convolutions\n",
    "    \n",
    "\n",
    "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 11, 11, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               51328     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 55,098\n",
      "Trainable params: 55,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.1742 - accuracy: 0.9473\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0569 - accuracy: 0.9822\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0411 - accuracy: 0.9868\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0307 - accuracy: 0.9902\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0238 - accuracy: 0.9924\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0200 - accuracy: 0.9934\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0156 - accuracy: 0.9947\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0116 - accuracy: 0.9961\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0085 - accuracy: 0.9973\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0366 - accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 113,386\n",
      "Trainable params: 113,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.1446 - accuracy: 0.9567\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0474 - accuracy: 0.9855\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0320 - accuracy: 0.9900\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0238 - accuracy: 0.9923\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0174 - accuracy: 0.9945\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0147 - accuracy: 0.9955\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0101 - accuracy: 0.9969\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0091 - accuracy: 0.9970\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0083 - accuracy: 0.9974\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0072 - accuracy: 0.9974\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0372 - accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.1237 - accuracy: 0.9617\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0409 - accuracy: 0.9871\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0276 - accuracy: 0.9912\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0204 - accuracy: 0.9932\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0143 - accuracy: 0.9952\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0113 - accuracy: 0.9965\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0103 - accuracy: 0.9966\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0080 - accuracy: 0.9975\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0064 - accuracy: 0.9977\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0076 - accuracy: 0.9977\n",
      "10000/10000 [==============================] - 1s 66us/sample - loss: 0.0358 - accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
    "\n",
    "    - Training time will decrease. Accuracy decreased as well. Minimal time difference ~10 $\\mu{s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "height": 415
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ZpYRidBXpBPM",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "70c1f9f2-880c-4923-9887-8f1d4c6b8383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.1692 - accuracy: 0.9505\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0580 - accuracy: 0.9829\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0374 - accuracy: 0.9890\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0260 - accuracy: 0.9917\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0183 - accuracy: 0.9941\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0107 - accuracy: 0.9969\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0080 - accuracy: 0.9972\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0060 - accuracy: 0.9981\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0055 - accuracy: 0.9983\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0559 - accuracy: 0.9858\n",
      "0.9858\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.1423 - accuracy: 0.9579\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0503 - accuracy: 0.9848\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0324 - accuracy: 0.9904\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0211 - accuracy: 0.9931\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0143 - accuracy: 0.9953\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0097 - accuracy: 0.9967\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0078 - accuracy: 0.9973\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0058 - accuracy: 0.9982\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0050 - accuracy: 0.9983\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0050 - accuracy: 0.9983\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0559 - accuracy: 0.9858\n",
      "0.9858\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.1377 - accuracy: 0.9592\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0474 - accuracy: 0.9863\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0274 - accuracy: 0.9916\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0184 - accuracy: 0.9941\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0127 - accuracy: 0.9956\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.0090 - accuracy: 0.9969\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0073 - accuracy: 0.9976\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0046 - accuracy: 0.9985\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0054 - accuracy: 0.9983\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.0039 - accuracy: 0.9987\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0614 - accuracy: 0.9858\n",
      "0.9858\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
    "\n",
    "    - It decreased training time on the first epoch. All later epochs increased in time by ~5 $\\mu{s}$\n",
    "    - Training accuracay was lower, but the test accuracy increased\n",
    "    - Adding even more convolutions further increased training time, however both training and test accuracy decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 149,642\n",
      "Trainable params: 149,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.1317 - accuracy: 0.9582\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.0435 - accuracy: 0.9866\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0308 - accuracy: 0.9905\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0227 - accuracy: 0.9930\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.0196 - accuracy: 0.9932\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0119 - accuracy: 0.9962\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0108 - accuracy: 0.9965\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0102 - accuracy: 0.9968\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0094 - accuracy: 0.9968\n",
      "10000/10000 [==============================] - 1s 62us/sample - loss: 0.0292 - accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),  \n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 1, 1, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 121,034\n",
      "Trainable params: 121,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.1508 - accuracy: 0.9530\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0490 - accuracy: 0.9845\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0371 - accuracy: 0.9886\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0283 - accuracy: 0.9913\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0209 - accuracy: 0.9934\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0195 - accuracy: 0.9936\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0148 - accuracy: 0.9955\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0135 - accuracy: 0.9959\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0107 - accuracy: 0.9967\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0120 - accuracy: 0.9962\n",
      "10000/10000 [==============================] - 1s 65us/sample - loss: 0.0434 - accuracy: 0.9893\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),  \n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.1266 - accuracy: 0.9610\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.0407 - accuracy: 0.9875\n",
      "Epoch 3/10\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0268 - accuracy: 0.9913\n",
      "Reached 99% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0268 - accuracy: 0.9913\n",
      "10000/10000 [==============================] - 1s 61us/sample - loss: 0.0325 - accuracy: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03251094119788177, 0.9901]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy') >= 0.99):\n",
    "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "print(tf.__version__)\n",
    "callback = myCallback()\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images,\n",
    "          training_labels,\n",
    "          epochs=10,\n",
    "          callbacks = [callback])\n",
    "test_loss = model.evaluate(test_images, test_labels)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Course 1 - Part 6 - Lesson 2 - Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
